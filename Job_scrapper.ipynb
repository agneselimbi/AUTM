{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7d6668d-772f-4e74-ab4f-7c10310f3261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver \n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException,TimeoutException,StaleElementReferenceException\n",
    "from os.path import exists\n",
    "import time\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re \n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import winsound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c86ac7ed-298b-46a9-bbf0-fec210fad5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_noise():\n",
    "  duration = 2000  # milliseconds\n",
    "  freq = 440  # Hz\n",
    "  winsound.Beep(freq, duration)\n",
    "\n",
    "## Helper functions\n",
    "\n",
    "### Formatting the date:\n",
    "def split_date(x):\n",
    "    m = re.search(r'\\d+[+]? \\w+ \\w+' ,str(x))\n",
    "    if m:\n",
    "        return m.group()\n",
    "    else:\n",
    "        return \" \"\n",
    "    \n",
    "def convert_date(x):\n",
    "    m = re.search(r'\\d+' ,str(x))\n",
    "    if m:\n",
    "        dt = datetime.today()-timedelta(int(m.group()))\n",
    "        dt = dt.strftime('%m/%d/%Y')  #time_dict)\n",
    "        return dt\n",
    "    else:\n",
    "        return \" \"\n",
    "    \n",
    "### Finding Zip and City\n",
    "def find_zip(x):\n",
    "    z = re.search(r'\\d+',x )\n",
    "    if z:\n",
    "         return(z.group())\n",
    "    else :\n",
    "        return \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc64729-8e94-4aaa-89d2-0f5a033d14e9",
   "metadata": {},
   "source": [
    "## Indeed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14098afe-b89c-4ca2-8eea-ce9df0963e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_indeed_jobs(keyword, location, n_pages):\n",
    "\n",
    "    url = f\"https://www.indeed.com/?from=gnav-homepage\"\n",
    "    ## Create an instance of the chrome webdriver. This is to interact with the website and test my code\n",
    "    s = Service ('C:\\Program Files (x86)\\chromedriver_win32\\chromedriver.exe')\n",
    "    wd = webdriver.Chrome(service = s)\n",
    "    wait = WebDriverWait(wd, 1)\n",
    "\n",
    "    ## Open Indeed Webpage \n",
    "    wd.get(url)\n",
    "\n",
    "    ## Enter search parameters\n",
    "    what_search = wd.find_element(By.ID, 'text-input-what')\n",
    "    what_search.send_keys(keyword)\n",
    "\n",
    "    where_search = wd.find_element(By.ID, 'text-input-where')\n",
    "    where_search.send_keys(Keys.CONTROL + \"a\")\n",
    "    where_search.send_keys(Keys.DELETE)\n",
    "    where_search.send_keys(location)\n",
    "    where_search.send_keys(Keys.RETURN)\n",
    "\n",
    "    base_url = wd.current_url\n",
    "\n",
    "    ## Initialise the different lists:\n",
    "    job_id =  []\n",
    "    job_title = []\n",
    "    seniority = []\n",
    "    emp_type=[]\n",
    "    job_link = []\n",
    "    industries = []\n",
    "    company_names = []\n",
    "    company_names2 = []\n",
    "    job_date =[]\n",
    "    job_location = []\n",
    "    job_salary = []\n",
    "    job_qual = []\n",
    "    job_description = []\n",
    "    \n",
    "    def extract(wait,job_id,job_title,seniority,emp_type,job_link,industries,company_names,company_names2,job_date,job_location,job_salary,job_qual,job_description):\n",
    "        i =1\n",
    "        results = wait.until(\n",
    "                        EC.presence_of_element_located((By.ID, \"mosaic-provider-jobcards\")))\n",
    "        jobs = results.find_elements(By.TAG_NAME,\"a\")\n",
    "        \n",
    "        for job in jobs:\n",
    "            try:\n",
    "                href = job.get_attribute(\"href\")\n",
    "                job_link.append(href)\n",
    "                #print(href)\n",
    "            except(NoSuchElementException, TimeoutException,StaleElementReferenceException): \n",
    "                job_link.append('')\n",
    "                #print('fail')\n",
    "\n",
    "            try :\n",
    "                ids = job.get_attribute(\"id\")\n",
    "                #print(ids)\n",
    "                job_id.append(ids)\n",
    "            except (NoSuchElementException, TimeoutException,StaleElementReferenceException): \n",
    "                job_id.append('')\n",
    "\n",
    "            try:\n",
    "                date = job.find_element(By.XPATH, '//span[@class = \"date\"]').text\n",
    "               # print('date', date)\n",
    "                job_date.append(date)\n",
    "            except (NoSuchElementException, TimeoutException,StaleElementReferenceException): \n",
    "                job_date.append(\"\")\n",
    "            \n",
    "            try:\n",
    "                company = job.find_element(By.XPATH,'//a[@data-tn-element = \"companyName\"]').get_attribute('text')\n",
    "                company_names.append(company) \n",
    "                #print(company)\n",
    "            except (NoSuchElementException, TimeoutException,StaleElementReferenceException): \n",
    "                company_names.append('')\n",
    "            \n",
    "            try:\n",
    "                loc = job.find_element(By.XPATH, '//div[@class = \"companyLocation\"]').text\n",
    "                #print(f'location, {loc}')                               \n",
    "                job_location.append(loc)\n",
    "            except (NoSuchElementException, TimeoutException,StaleElementReferenceException): \n",
    "                job_location.append('')\n",
    "                print('location fail')\n",
    "\n",
    "            try:\n",
    "                results.find_element(By.XPATH,f'//*[@id=\"mosaic-provider-jobcards\"]/ul/li[{i}]/div').click()\n",
    "                time.sleep(.5)\n",
    "                iframe =results.find_element(By.XPATH,'//*[@id=\"vjs-container\"]').find_element(By.CSS_SELECTOR,'iframe')\n",
    "                wd.switch_to.frame(iframe)\n",
    "                ## Find the job's title \n",
    "                try:\n",
    "                    title = wait.until(EC.presence_of_element_located((By.TAG_NAME,\"h1\"))).get_attribute(\"textContent\")\n",
    "                    title = title.replace(\" - job post\", \"\")\n",
    "                    job_title.append(title)\n",
    "                    #print(title)   \n",
    "                except (NoSuchElementException, TimeoutException,StaleElementReferenceException): \n",
    "                    job_title.append('')\n",
    "\n",
    "                try:\n",
    "                    salary  = wait.until(\n",
    "                                EC.presence_of_element_located((By.XPATH,'//*[@id=\"jobDetailsSection\"]/div[2]/span'))).get_attribute('innerText')\n",
    "                    job_salary.append(salary)\n",
    "                    #print(f'salary,{salary}')\n",
    "                except (NoSuchElementException, TimeoutException,StaleElementReferenceException): \n",
    "                    job_salary.append('')\n",
    "\n",
    "                try:\n",
    "                    emp = wait.until(\n",
    "                                EC.presence_of_element_located((By.XPATH, '//*[@id=\"jobDetailsSection\"]/div[3]/div[2]'))).get_attribute('innerText')             \n",
    "                    emp_type.append(emp)\n",
    "                    #print(\"employment type  , \", emp)\n",
    "                except (NoSuchElementException, TimeoutException,StaleElementReferenceException): \n",
    "                    emp_type.append(\"\")\n",
    "\n",
    "                try:\n",
    "                    qual = wait.until(\n",
    "                                EC.presence_of_element_located((By.CLASS_NAME, \"jobsearch-ReqAndQualSection-item--wrapper\"))).get_attribute('innerText')\n",
    "                    #print('qual',qual)\n",
    "                    job_qual.append(qual)\n",
    "                except (NoSuchElementException, TimeoutException,StaleElementReferenceException):\n",
    "                    job_qual.append('')\n",
    "\n",
    "                try:\n",
    "                    summary = wait.until(\n",
    "                                EC.presence_of_element_located((By.XPATH,'//*[@id=\"jobDescriptionText\"]'))).get_attribute('innerText')\n",
    "                    #print(f'summary,{summary}')\n",
    "                    job_description.append(summary)\n",
    "                except (NoSuchElementException, TimeoutException,StaleElementReferenceException):\n",
    "                    job_description.append('')\n",
    "\n",
    "                wd.switch_to.default_content()      \n",
    "            except:\n",
    "                job_title.append('')\n",
    "                emp_type.append('')\n",
    "                job_salary.append('')\n",
    "                job_qual.append('')\n",
    "                job_description.append('') \n",
    "\n",
    "            i+=1\n",
    "        return job_id,job_title,seniority,emp_type,job_link,industries,company_names,company_names2,job_date,job_location,job_salary,job_qual,job_description\n",
    "    \n",
    "    ## Iterate over the different pages \n",
    "    i=1\n",
    "    count = 1\n",
    "\n",
    "    try:\n",
    "        wait.until(EC.element_to_be_clickable((By.ID,'popover-x'))).click()       \n",
    "    except (NoSuchElementException, TimeoutException,StaleElementReferenceException): \n",
    "        print(\"no pop up\")\n",
    "\n",
    "    job_id,job_title,seniority,emp_type,job_link,industries,company_names,company_names2,job_date,job_location,job_salary,job_qual,job_description = extract(wait,job_id,job_title,seniority,emp_type,job_link,industries,company_names,company_names2,job_date,job_location,job_salary,job_qual,job_description)\n",
    "    while i<= n_pages:\n",
    "        wd.execute_script(\"window.scrollTo(0,document.body.scrollHeight);\")\n",
    "        try:\n",
    "            #time.sleep(1)\n",
    "            button = wd.find_element(By.XPATH,f'//*[@id=\"resultsCol\"]/nav/div/ul/li[{i}]/a').click()\n",
    "            try:\n",
    "                wait.until(EC.element_to_be_clickable((By.ID,'popover-x'))).click()       \n",
    "            except (NoSuchElementException, TimeoutException,StaleElementReferenceException): \n",
    "                print(\"no pop up\")\n",
    "            job_id,job_title,seniority,emp_type,job_link,industries,company_names,job_date,job_location,job_salary,job_qual,job_description = extract(wait,job_id,job_title,seniority,emp_type,job_link,industries,company_names,company_names2,job_date,job_location,job_salary,job_qual,job_description)\n",
    "            time.sleep(1)\n",
    "            #wd.back()\n",
    "            count += 1\n",
    "        except:\n",
    "            print(\"End of pages scraped\")\n",
    "        i+=1\n",
    "\n",
    "    ## Cleaning the dataframe \n",
    "    job_list = {'ID' : job_id,\n",
    "                'Title': job_title,\n",
    "                'Date': job_date, \n",
    "                'Company': company_names,\n",
    "                #'Company2': company_names2,\n",
    "                'Location': job_location,\n",
    "                'Salary': job_salary,\n",
    "                #'Seniority': seniority,\n",
    "                'Employment Type': emp_type,\n",
    "                #'Function': job_func,\n",
    "                #'Industry': industries,\n",
    "                'Qualification': job_qual,\n",
    "                'Job description':job_description,\n",
    "                'Link': job_link}\n",
    "\n",
    "    df = pd.DataFrame(job_list)\n",
    "    df = df.replace('',np.nan)\n",
    "    df= df.dropna(subset=['Title'])\n",
    "    df['City'] = df['Location'].apply(lambda x: str(x).split(',')[0])\n",
    "    df['Date Posted'] = df['Date'].apply(lambda x: convert_date(x))    \n",
    "    df['Date'] = df['Date'].apply(lambda x: split_date(x))\n",
    "\n",
    "\n",
    "    ## Homogenize the data frame:\n",
    "    df['Zip Code'] = np.nan\n",
    "    df['Seniority'] = np.nan\n",
    "    df['Function'] = np.nan\n",
    "    df['Industry'] = np.nan\n",
    "    df['Source'] = 'Indeed'\n",
    "    \n",
    "\n",
    "    cols = ['Title', 'Date', 'Date Posted', 'Company','Location',\n",
    "                'Zip Code' ,'Salary','Employment Type', 'Qualification', 'Seniority', 'Function', 'Industry','Job description', 'Link', 'Source' ]\n",
    "    df= df[cols]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a45c34-84cd-4146-bc73-ccd395addf90",
   "metadata": {},
   "source": [
    "## Monster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49cddcf0-57f5-40de-8e02-0a273a7ecc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_monster_jobs(keyword, location, n_pages):\n",
    "    url = f\"https://www.monster.com/jobs\"\n",
    "\n",
    "    ## Create an instance of the webdriver \n",
    "    s = Service('C:\\Program Files (x86)\\chromedriver_win32\\chromedriver.exe')\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument( \"--window-size=1000,1100\")\n",
    "    wd = webdriver.Chrome(service = s, options = chrome_options)\n",
    "\n",
    "    ## Open the Monster Webpage \n",
    "    wd.get(url)\n",
    "    wait = WebDriverWait(wd,2)\n",
    "\n",
    "    ## Input the Job title and location\n",
    "    what_search = wd.find_element(By.ID, \"search-job\" )\n",
    "    what_search.send_keys(keyword)\n",
    "    where_search = wd.find_element(By.ID, \"search-location\")\n",
    "    where_search.send_keys(location)\n",
    "    where_search.send_keys(Keys.RETURN)\n",
    "    time.sleep(1)\n",
    "\n",
    "    def infinite_scroll():  \n",
    "        SCROLL_PAUSE_TIME = 1\n",
    "        # Get scroll height\n",
    "        last_height = wd.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        while True:\n",
    "            # Scroll down to bottom\n",
    "            wd.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "            # Wait to load page\n",
    "            time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "            # Calculate new scroll height and compare with last scroll height\n",
    "            new_height = wd.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "\n",
    "    i=1\n",
    "    infinite_scroll()\n",
    "    wd.execute_script(\"window.scrollTo(0,900);\")\n",
    "    while (i <= n_pages):\n",
    "        infinite_scroll()\n",
    "        wait.until(EC.presence_of_element_located((By.XPATH,'//button[@class =\"sc-eCImPb dVaZLw  ds-button\"]'))).click()  \n",
    "        i+=1\n",
    "\n",
    "    wd.switch_to.window(wd.window_handles[0])\n",
    "    jobs = wd.find_element(By.XPATH,'//*[@id=\"__next\"]/div[3]/main/div[2]/nav/section[1]/div[2]/div/div/div').find_elements(By.CSS_SELECTOR,'div')\n",
    "    n_jobs = len(jobs)\n",
    "\n",
    "    time.sleep(1)\n",
    "    job_id =  []\n",
    "    job_title = []\n",
    "    seniority = []\n",
    "    emp_type=[]\n",
    "    job_link = []\n",
    "    industries = []\n",
    "    company_names = []\n",
    "    job_date =[]\n",
    "    job_location = []\n",
    "    job_salary = []\n",
    "    job_qual = []\n",
    "    job_description = []\n",
    "    act_url = wd.current_url\n",
    "    jobs = wd.find_element(By.XPATH,'//*[@id=\"__next\"]/div[3]/main/div[2]/nav/section[1]/div[2]/div/div/div').find_elements(By.CSS_SELECTOR,'div')\n",
    "    n_jobs = len(jobs)\n",
    "    wd.execute_script(\"window.scrollTo(0,document.body.scrollHeight);\")\n",
    "    print(\"len_jobs\", n_jobs)\n",
    "\n",
    "    for i in range(1,n_jobs):         \n",
    "        try:\n",
    "            job = wd.find_element(By.XPATH, f'//*[@id=\"__next\"]/div[3]/main/div[2]/nav/section[1]/div[2]/div/div/div/div[{i}]').click()\n",
    "            wd.switch_to.window(wd.window_handles[1])\n",
    "            try:\n",
    "                title = wd.find_element(By.TAG_NAME, \"h1\").text\n",
    "                job_title.append(title)\n",
    "                #print(title)\n",
    "            except:\n",
    "                job_title.append('')\n",
    "            try:\n",
    "                date = wd.find_element(By.XPATH, \"//div[@data-test-id='svx-jobview-posted']\").text\n",
    "                #print(date)\n",
    "                job_date.append(date)\n",
    "            except:\n",
    "                job_date.append('')\n",
    "            try:\n",
    "                company = wd.find_element(By.TAG_NAME, \"h2\").text\n",
    "                company_names.append(company)\n",
    "            except:\n",
    "                company_names.append('')\n",
    "                #print(company)\n",
    "            try:\n",
    "                location =  wd.find_element(By.XPATH, \"//div[@data-test-id = 'svx-jobview-location']\").text\n",
    "                job_location.append(location)\n",
    "            except:\n",
    "                job_location.append('')\n",
    "                    #print(location)\n",
    "            try:\n",
    "                link = wd.find_element(By.CSS_SELECTOR, 'a').get_attribute('href')\n",
    "                job_link.append(link)\n",
    "            except:\n",
    "                    job_link.append('')\n",
    "                    print(link)\n",
    "            try:\n",
    "                salary= wd.find_element(By.XPATH, \"//span[@data-testid ='svx_salaryComponent_body']\").text\n",
    "                #salary = wd.find_element(By.CLASS_NAME, \"salarystyle__SalaryBody-sc-1kub5et-8 jeLUTC\").text\n",
    "                job_salary.append(salary)\n",
    "            except:\n",
    "                job_salary.append('')\n",
    "\n",
    "            try:\n",
    "                emp =wd.find_element(By.XPATH, \"//div[@data-test-id='svx-jobview-employmenttype']\").text\n",
    "                emp_type.append(emp)\n",
    "            except:\n",
    "                emp_type.append('')\n",
    "\n",
    "            try:\n",
    "                description = wd.find_element(By.XPATH,'//*[@id=\"jobview-container\"]/div[1]/div/div[2]/div').text\n",
    "                job_description.append(description)\n",
    "                #print(description)\n",
    "            except:\n",
    "                job_description.append('')\n",
    "\n",
    "\n",
    "            wd.close() \n",
    "            wd.switch_to.window(wd.window_handles[0]) \n",
    "            time.sleep(1)\n",
    "\n",
    "        except:\n",
    "            print('fail')\n",
    "            pass\n",
    "        \n",
    "     ## Clean out the dataframe \n",
    "    job_list = {#'ID' : job_id,\n",
    "                'Title': job_title,\n",
    "                'Date': job_date, \n",
    "                'Company': company_names,\n",
    "                'Location': job_location,\n",
    "                'Salary': job_salary,\n",
    "                #'Seniority': seniority,\n",
    "                'Employment Type': emp_type,\n",
    "                #'Function': job_func,\n",
    "                #'Industry': industries,\n",
    "                #'Qualification': job_qual,\n",
    "                'Job description':job_description,\n",
    "                'Link': job_link}\n",
    "    df = pd.DataFrame(job_list)\n",
    "\n",
    "     ## Remove empty lines \n",
    "    df = df.replace('',np.nan)\n",
    "    df= df.dropna(subset=['Title'])\n",
    "\n",
    "    # Convert the date\n",
    "    df['Date Posted'] = df['Date'].apply(lambda x: convert_date(x))\n",
    "    df['Date'] = df['Date'].apply(lambda x: split_date(x))\n",
    "\n",
    "    ## Add variables that don't exsist in the website \n",
    "    df['ID'] = np.nan\n",
    "    df['Function'] = np.nan\n",
    "    df['Industry']= np.nan\n",
    "    df['Qualification'] = np.nan\n",
    "    df['Seniority'] = np.nan\n",
    "    df['Industry'] = np.nan\n",
    "    df['Zip Code'] = np.nan\n",
    "    df['Company2'] = np.nan\n",
    "    df['Source'] = 'Monster'\n",
    "    \n",
    "    cols = [ 'Title', 'Date', 'Date Posted', 'Company', 'Company2','Location',\n",
    "            'Zip Code' ,'Salary','Employment Type', 'Qualification', 'Seniority', 'Function', 'Industry','Job description', 'Link','Source' ]\n",
    "    df= df[cols]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d765e7e1-d512-4646-a98f-480bf2a46731",
   "metadata": {},
   "source": [
    "## Linkedin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6c12c89-74f2-432e-8a22-a2add7f152e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_linkedin_jobs(keyword,place,n_pages):\n",
    "    s = Service ('C:\\Program Files (x86)\\chromedriver_win32\\chromedriver.exe')\n",
    "    wd = webdriver.Chrome(service = s)\n",
    "    wait = WebDriverWait(wd, 1)\n",
    "\n",
    "    ## change User Agent at each request \n",
    "    username = \"agneselimbi@yahoo.fr\" \n",
    "    password = \"Voldemort_1961\" # don't forget to remove when handing over \n",
    "    url = \"https://www.linkedin.com/\"\n",
    "    wd.get(url)\n",
    "\n",
    "\n",
    "    ## Sign_in in LinkedIn \n",
    "    signin = wd.find_element(By.XPATH, '/html/body/nav/div/a[2]').click()\n",
    "    ids = wd.find_element(By.ID, \"username\")\n",
    "    ids.send_keys (username)\n",
    "    code =  wd.find_element(By.ID, \"password\") \n",
    "    code.send_keys(password)\n",
    "    code.send_keys(Keys.RETURN)\n",
    "    wd.find_element(By.XPATH,'//*[@id=\"ember19\"]').click()\n",
    "    time.sleep(1)\n",
    "    what_search = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"input[id^='jobs-search-box-keyword']\")))\n",
    "    what_search.send_keys(keyword)\n",
    "    time.sleep(1)\n",
    "    where_search =  wd.find_element(By.CSS_SELECTOR, \"input[id^= 'jobs-search-box-location-id']\")\n",
    "    time.sleep(1)\n",
    "    where_search.send_keys(place)\n",
    "    what_search.send_keys(Keys.RETURN)\n",
    "    time.sleep(1)\n",
    "\n",
    "    ## Initialize the different lists\n",
    "    job_id = []\n",
    "    job_title = []\n",
    "    company = []\n",
    "    job_location = []\n",
    "    job_salary = []\n",
    "    job_date = []\n",
    "    job_link = []\n",
    "    job_description = []\n",
    "    seniority = []\n",
    "    emp_type=[]\n",
    "    job_func = []\n",
    "    industries = []\n",
    "    company_names = []\n",
    "\n",
    "    ## Iterate over the number of pages:\n",
    "    page = 1   \n",
    "    url = wd.current_url\n",
    "    while page <= n_pages:       \n",
    "        try: \n",
    "            wd.maximize_window()\n",
    "            time.sleep(1)\n",
    "            wd.execute_script(\"window.scrollTo(0,document.body.scrollHeight);\")\n",
    "            time.sleep(1)\n",
    "            wait.until(EC.presence_of_element_located((By.XPATH,f'/html/body/div[6]/div[3]/div[3]/div[2]/div/section[1]/div/div/section/div/ul/li[{page}]/button'))).click()  \n",
    "            #print(f\"page {page}\")\n",
    "\n",
    "            ## Find the number of jobs on one page:\n",
    "            job_list = wd.find_element(By.CLASS_NAME,'jobs-search-results__list')\n",
    "            jobs = job_list.find_elements(By.CLASS_NAME,'jobs-search-results__list-item')\n",
    "            n_jobs = len(jobs)\n",
    "\n",
    "            i= 1\n",
    "            for job in jobs:\n",
    "                #print(i)\n",
    "                url = wd.current_url\n",
    "                try:\n",
    "                    ids = job.get_attribute(\"data-job-id\")\n",
    "                    job_id.append(ids)\n",
    "                except:\n",
    "                    job_id.append('')\n",
    "                    #print('fail')\n",
    "\n",
    "                try:\n",
    "                    title = wait.until(EC.presence_of_element_located((By.XPATH, f'/html/body/div[6]/div[3]/div[3]/div[2]/div/section[1]/div/div/ul/li[{i}]/div/div[1]/div[1]/div[2]/div[1]/a'))).text\n",
    "                    job_title.append(title)\n",
    "                except:\n",
    "                    job_title.append('')\n",
    "                    #print('title fail')\n",
    "\n",
    "                try:\n",
    "                    comp= wait.until(EC.presence_of_element_located((By.XPATH, f'/html/body/div[6]/div[3]/div[3]/div[2]/div/section[1]/div/div/ul/li[{i}]/div/div[1]/div[1]/div[2]/div[2]/a'))).text\n",
    "                    company_names.append(comp)\n",
    "                except:\n",
    "                    company_names.append('')\n",
    "                    #print('company fail')\n",
    "\n",
    "                try:\n",
    "                    location = wait.until(EC.presence_of_element_located((By.XPATH, f'/html/body/div[6]/div[3]/div[3]/div[2]/div/section[1]/div/div/ul/li[{i}]/div/div[1]/div[1]/div[2]/div[3]/ul/li[1]'))).text  \n",
    "                    job_location.append(location)\n",
    "                except:\n",
    "                    job_location.append('')\n",
    "                    #print('location fail')\n",
    "\n",
    "                try:\n",
    "                    uid = job.get_attribute(\"id\")\n",
    "                    path = r'//*[@id = \"{}\"]/div/div'.format(uid)\n",
    "                    wd.find_element(By.XPATH,path).click()\n",
    "                    cur_url = wd.current_url\n",
    "                    time.sleep(3)\n",
    "                    try:\n",
    "                        emp = wait.until(EC.presence_of_element_located((By.XPATH, f'/html/body/div[6]/div[3]/div[3]/div[2]/div/section[2]/div/div/div[1]/div/div[1]/div/div[2]/div[2]/ul/li[1]/span'))).text\n",
    "                        emp_type.append(emp)\n",
    "                    except:\n",
    "                        emp_type.apppend('')    \n",
    "                        #print('employment fail')\n",
    "                    try:                         \n",
    "                        link = wait.until(EC.presence_of_element_located((By.XPATH, f'/html/body/div[6]/div[3]/div[3]/div[2]/div/section[2]/div/div/div[1]/div/div[1]/div/div[2]/a'))).get_attribute('href')\n",
    "                        job_link.append(link)\n",
    "                    except:\n",
    "                        job_link.append('')\n",
    "                        #print('link fail')\n",
    "\n",
    "                    try:\n",
    "                        industry = wait.until(EC.presence_of_element_located((By.XPATH,'/html/body/div[6]/div[3]/div[3]/div[2]/div/section[2]/div/div/div[1]/div/div[1]/div/div[2]/div[2]/ul/li[2]/span'))).text\n",
    "                        industries.append(industry)     \n",
    "                    except:\n",
    "                        industries.append('')\n",
    "                        #print('industry fail')\n",
    "\n",
    "                    try:\n",
    "                        age = wait.until(EC.presence_of_element_located((By.XPATH, '/html/body/div[6]/div[3]/div[3]/div[2]/div/section[2]/div/div/div[1]/div/div[1]/div/div[2]/div[1]/span[2]/span[1]'))).text\n",
    "                        job_date.append(age)\n",
    "                    except:\n",
    "                        job_date.append('')\n",
    "                        #print('date fail')\n",
    "\n",
    "                    try:\n",
    "                        description = wait.until(EC.presence_of_element_located((By.XPATH,'/html/body/div[6]/div[3]/div[3]/div[2]/div/section[2]/div/div/div[1]/div/div[2]/article/div/div[1]/span'))).text\n",
    "                        job_description.append(description)\n",
    "                    except:\n",
    "                        job_description.append('')\n",
    "                        #print('description fail')\n",
    "\n",
    "                except:\n",
    "                    #print('fail')\n",
    "                    job_link.append('')\n",
    "                    job_date.append('')\n",
    "                    job_description.append('')\n",
    "                    emp_type.append('')\n",
    "                    industries.append('')\n",
    "\n",
    "                if cur_url != url:\n",
    "                    wd.back()\n",
    "                i= i+1    \n",
    "\n",
    "        except:\n",
    "            pass\n",
    "        page = page+1\n",
    "        ##print(f'page{page}')\n",
    "\n",
    "\n",
    "        ## Create a dataframe\n",
    "    #print(len(job_id),len(job_title),len(company_names),len(job_location),len(emp_type),len(industries),len(job_link),len(job_description))\n",
    "    df = pd.DataFrame({ 'ID' : job_id,\n",
    "                                 'Title': job_title,\n",
    "                                 'Date Posted': job_date, \n",
    "                                 'Company': company_names,\n",
    "                                 'Location': job_location,\n",
    "                                 'Employment Type': emp_type,\n",
    "                                 'Industry': industries,\n",
    "                                 'Link': job_link,\n",
    "                                 'Job description': job_description\n",
    "                            })\n",
    "\n",
    "    df['Zip Code'] = np.nan\n",
    "    df['Date'] = np.nan\n",
    "    df['Qualification'] = np.nan\n",
    "    df['Salary'] = np.nan\n",
    "    df['Source'] = 'LinkedIn'\n",
    "\n",
    "    cols = [ 'Title', 'Date', 'Date Posted', 'Company','Location',\n",
    "            'Zip Code' ,'Salary','Employment Type', 'Qualification', 'Industry','Job description', 'Link','Source']\n",
    "\n",
    "    df= df[cols]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2467083",
   "metadata": {},
   "source": [
    "# Scraping files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea0873cc-a242-4b42-ae9a-7e154b2dc3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_jobs():\n",
    "    ## Merge scrapped data in one file\n",
    "    linkedin_df = scrape_linkedin_jobs(keyword,place,n_pages)\n",
    "    indeed_df= scrape_indeed_jobs(keyword, place , n_pages)\n",
    "    monster_df= scrape_monster_jobs(keyword, place , n_pages)\n",
    "    make_noise()\n",
    "\n",
    "    df = pd.concat([linkedin_df,monster_df,indeed_df])\n",
    "\n",
    "    ## Remove the duplicates from the total dataframe \n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    ## Export cleanead data frame to our master list \n",
    "    if exists(\"Master_job_list.csv\"):\n",
    "        df.to_csv(\"Master_job_list.csv\", mode = 'a', index = False, header = False) \n",
    "    else:\n",
    "        df.to_csv(\"Master_job_list.csv\", index = False, header = True)   ## first run of code\n",
    "    \n",
    "    df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22a1b437-b026-47b2-afa2-6290fcfabdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_master_df():\n",
    "    master_df = pd.read_csv(\"Master_job_list.csv\")\n",
    "        \n",
    "    # Remove jobs that are too old (posted more than 2 weeks ago)\n",
    "    master_df = master_df[(master_df['Date Posted'] > (date.today() -timedelta(14)).strftime('%m/%d/%Y')) & (master_df['Date Posted'] <=  (date.today()+timedelta(14)).strftime('%m/%d/%Y'))]\n",
    "    master_df.reset_index()    \n",
    "    master_df.fillna('')\n",
    "\n",
    "    # Discretize the job description\n",
    "    def clean(text):\n",
    "        # removing new line characters\n",
    "        text = re.sub('\\n ','',str(text))\n",
    "        text = re.sub('\\n',' ',str(text))\n",
    "        # removing apostrophes\n",
    "        text = re.sub(\"'s\",'',str(text))\n",
    "        # removing hyphensM\n",
    "        text = re.sub(\"-\",' ',str(text))\n",
    "        text = re.sub(\"— \",'',str(text))\n",
    "        # removing quotation marks\n",
    "        text = re.sub('\\\"','',str(text))\n",
    "        # removing any reference to outside text\n",
    "        text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", str(text))   \n",
    "        return text.lower()\n",
    "\n",
    "    def find_salary(txt):\n",
    "        salary_pattern = re.compile(r'(\\$\\d+,?\\d+)')\n",
    "        return salary_pattern.findall(txt)\n",
    "\n",
    "    def find_qualification(txt):\n",
    "        return ([sentence + '.' for sentence in txt.split('.') if (\"qualification\") in sentence])\n",
    "\n",
    "    def find_education(txt):\n",
    "        return ([sentence + '.' for sentence in txt.split('.') if (\"bachelor\"or\"master\") in sentence])\n",
    "\n",
    "    master_df['cleaned job'] = master_df[\"Job description\"].apply(lambda x: clean(x))\n",
    "    master_df['extracted salary'] = master_df['cleaned job'].apply(lambda x: find_salary(x) )\n",
    "    master_df['extracted education'] = master_df['cleaned job'].apply(lambda x: find_education(x) )\n",
    "    master_df['extracted qualification'] = master_df['cleaned job'].apply(lambda x: find_qualification(x) )\n",
    "    \n",
    "     # Save master_df\n",
    "    master_df.to_csv(\"Cleaned_Master_job_list.csv\",index = False,header =True)\n",
    "    return master_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d8e8538-64d5-4f21-b86a-ce15cbef1519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the postions you are scraping : Machine Learning Engineer\n",
      "Enter the job's location:  Austin,TX\n",
      "Enter the number of pages you want to scrape: 1\n"
     ]
    },
    {
     "ename": "ElementNotInteractableException",
     "evalue": "Message: element not interactable\n  (Session info: chrome=101.0.4951.67)\nStacktrace:\nBacktrace:\n\tOrdinal0 [0x00B57413+2389011]\n\tOrdinal0 [0x00AE9F61+1941345]\n\tOrdinal0 [0x009DC520+836896]\n\tOrdinal0 [0x00A048E3+1001699]\n\tOrdinal0 [0x00A03FBE+999358]\n\tOrdinal0 [0x00A2414C+1130828]\n\tOrdinal0 [0x009FF974+981364]\n\tOrdinal0 [0x00A24364+1131364]\n\tOrdinal0 [0x00A34302+1196802]\n\tOrdinal0 [0x00A23F66+1130342]\n\tOrdinal0 [0x009FE546+976198]\n\tOrdinal0 [0x009FF456+980054]\n\tGetHandleVerifier [0x00D09632+1727522]\n\tGetHandleVerifier [0x00DBBA4D+2457661]\n\tGetHandleVerifier [0x00BEEB81+569713]\n\tGetHandleVerifier [0x00BEDD76+566118]\n\tOrdinal0 [0x00AF0B2B+1968939]\n\tOrdinal0 [0x00AF5988+1989000]\n\tOrdinal0 [0x00AF5A75+1989237]\n\tOrdinal0 [0x00AFECB1+2026673]\n\tBaseThreadInitThunk [0x77266739+25]\n\tRtlGetFullPathName_UEx [0x77E48FEF+1215]\n\tRtlGetFullPathName_UEx [0x77E48FBD+1165]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mElementNotInteractableException\u001b[0m           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-11615fb7b3f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Enter the job's location:  \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mn_pages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Enter the number of pages you want to scrape: \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mscrape_jobs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mupdate_master_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-cfa69c758309>\u001b[0m in \u001b[0;36mscrape_jobs\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mscrape_jobs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m## Merge scrapped data in one file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mlinkedin_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscrape_linkedin_jobs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyword\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mplace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_pages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mindeed_df\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mscrape_indeed_jobs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplace\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mn_pages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmonster_df\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mscrape_monster_jobs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplace\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mn_pages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-3fc34b8be4d2>\u001b[0m in \u001b[0;36mscrape_linkedin_jobs\u001b[1;34m(keyword, place, n_pages)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mwhere_search\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mwd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCSS_SELECTOR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"input[id^= 'jobs-search-box-location-id']\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mwhere_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[0mwhat_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRETURN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py\u001b[0m in \u001b[0;36msend_keys\u001b[1;34m(self, *value)\u001b[0m\n\u001b[0;32m    538\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremote_files\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 540\u001b[1;33m         self._execute(Command.SEND_KEYS_TO_ELEMENT,\n\u001b[0m\u001b[0;32m    541\u001b[0m                       {'text': \"\".join(keys_to_typing(value)),\n\u001b[0;32m    542\u001b[0m                        'value': keys_to_typing(value)})\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py\u001b[0m in \u001b[0;36m_execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    708\u001b[0m             \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m         \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 710\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    711\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfind_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 424\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    425\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[0;32m    426\u001b[0m                 response.get('value', None))\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    245\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alert'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mMapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_KT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_VT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_KT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_VT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0m_VT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mElementNotInteractableException\u001b[0m: Message: element not interactable\n  (Session info: chrome=101.0.4951.67)\nStacktrace:\nBacktrace:\n\tOrdinal0 [0x00B57413+2389011]\n\tOrdinal0 [0x00AE9F61+1941345]\n\tOrdinal0 [0x009DC520+836896]\n\tOrdinal0 [0x00A048E3+1001699]\n\tOrdinal0 [0x00A03FBE+999358]\n\tOrdinal0 [0x00A2414C+1130828]\n\tOrdinal0 [0x009FF974+981364]\n\tOrdinal0 [0x00A24364+1131364]\n\tOrdinal0 [0x00A34302+1196802]\n\tOrdinal0 [0x00A23F66+1130342]\n\tOrdinal0 [0x009FE546+976198]\n\tOrdinal0 [0x009FF456+980054]\n\tGetHandleVerifier [0x00D09632+1727522]\n\tGetHandleVerifier [0x00DBBA4D+2457661]\n\tGetHandleVerifier [0x00BEEB81+569713]\n\tGetHandleVerifier [0x00BEDD76+566118]\n\tOrdinal0 [0x00AF0B2B+1968939]\n\tOrdinal0 [0x00AF5988+1989000]\n\tOrdinal0 [0x00AF5A75+1989237]\n\tOrdinal0 [0x00AFECB1+2026673]\n\tBaseThreadInitThunk [0x77266739+25]\n\tRtlGetFullPathName_UEx [0x77E48FEF+1215]\n\tRtlGetFullPathName_UEx [0x77E48FBD+1165]\n"
     ]
    }
   ],
   "source": [
    "## Input the keywords \n",
    "keyword = input(\"Enter the postions you are scraping : \")\n",
    "place = input (\"Enter the job's location:  \")\n",
    "n_pages = int(input(\"Enter the number of pages you want to scrape: \"))\n",
    "scrape_jobs()\n",
    "update_master_df()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bec84d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
